{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:white;\n",
    "            display:fill;\n",
    "            border-radius:20px;\n",
    "            background-color:green;\n",
    "            font-size:200%;\n",
    "            font-family:Verdana;\n",
    "            letter-spacing:1px\">\n",
    "    <h1 style='padding: 10px;\n",
    "               margin:0px auto 0px auto;\n",
    "              color:white;\n",
    "              text-align:center;'>\n",
    "       &nbsp;Depth Prediction with MiDaS\n",
    "    </h1>\n",
    "    </div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the Reference code we have created and not the actual usable code. <br> \n",
    "### (Works on my system) (Needs different config for different computer)\n",
    "#### One can easily edit the code for their paths and make it run for their system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n",
    "#model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\n",
    "#model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n",
    "\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "\n",
    "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
    "    transform = midas_transforms.dpt_transform\n",
    "else:\n",
    "    transform = midas_transforms.small_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "# Path to the folder containing YOLO annotated files\n",
    "anno_folder = '/path/to/annotated/files'\n",
    "\n",
    "# Path to the Midas depth image\n",
    "midas_img_path = '/path/to/midas/image'\n",
    "\n",
    "# Create an empty dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Define the density of the material in cm^3/g (hand crafted)\n",
    "density = {'Aluminium foil': 2.7,'Battery': 1.5,'Aluminium blister pack':2.71,'Carded blister pack': 2.4,'Other plastic bottle': 1.6,'Clear plastic bottle': 1,'Glass bottle': 3.2,'Plastic bottle cap': 0.92,'Metal bottle cap': 1.3,'Broken glass': 3.2,'Food Can': 3.6,'Aerosol': 2,'Drink can': 1,'Toilet tube': 2.2,'Other carton': 1.6,'Egg carton': 1.4,'Drink carton': 1.7,'Corrugated carton': 0.5,'Meal carton': 1.4,'Pizza box': 1.8,'Paper cup': 0.8,'Disposable plastic cup': 0.8,'Foam cup':0.6,'Glass cup': 2.2,'Other plastic cup': 0.8,'Food waste': 1.4,'Glass jar':2.4,'Plastic lid': 0.8,'Metal lid': 1.2,'Other plastic': 0.9,'Magazine paper': 0.3,'Tissues': 0.3,'Wrapping paper': 0.5,'Normal paper': 0.7,'Paper bag':0.7,'Plastified paper bag': 0.7,'Plastic film' : 1.2,'Six pack rings': 1.3,'Garbage bag': 4,'Other plastic wrapper' : 1.2,'Single-use carrier bag': 1.2,'Polypropylene bag': 1.3,'Crisp packet': 2.2,'Spread tub' : 3.4,'Tupperware' : 4.2,'Disposable food container':2.7,'Foam food container': 1.7,'Other plastic container': 1.7,'Plastic glooves': 2.2,'Plastic utensils': 1.9,'Pop tab': 0.4,'Rope & strings': 0.2,'Scrap metal' : 4.3,'Shoe' : 3.8,'Squeezable tube': 1.7,'Plastic straw' : 0.8,'Paper straw': 0.6,'Styrofoam piece' : 2.2,'Unlabeled litter': 2,'Cigarette': 4.2}\n",
    "\n",
    "# Define a constant factor to scale the weight calculation (set according to comparitions of the results with the ground truth)\n",
    "scale_factor = 0.001\n",
    "\n",
    "# Initialize a variable to store the total weight of the image\n",
    "total_weight = 0\n",
    "\n",
    "# Loop through the annotated files in the folder\n",
    "for filename in os.listdir(anno_folder):\n",
    "    if filename.endswith('.jpg'):\n",
    "        # Read input image\n",
    "        img = cv2.imread(os.path.join(anno_folder, filename))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        input_batch = transform(img).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = midas(input_batch)\n",
    "\n",
    "            prediction = torch.nn.functional.interpolate(\n",
    "                prediction.unsqueeze(1),\n",
    "                size=img.shape[:2],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            ).squeeze()\n",
    "\n",
    "        output = prediction.cpu().numpy()\n",
    "\n",
    "        # Save output image\n",
    "        save_filename = os.path.join(midas_img_path, os.path.splitext(filename)[0] + '_midas')\n",
    "        plt.imsave(save_filename + '.png', output, cmap='gray')\n",
    "\n",
    "    if filename.endswith('.txt'):\n",
    "        # Load the YOLO annotation file\n",
    "        anno_file = os.path.join(anno_folder, filename)\n",
    "        with open(anno_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Extract the coordinates of all the annotated areas\n",
    "        class_coords = {}\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                class_id, x1, y1, x2, y2 = [int(x) for x in line.split()]\n",
    "                if class_id not in class_coords:\n",
    "                    class_coords[class_id] = []\n",
    "                class_coords[class_id].append((x1, y1, x2, y2))\n",
    "\n",
    "        # Calculate the mode of the pixel values for each class separately\n",
    "        file_results = {}\n",
    "        for class_id, coords in class_coords.items():\n",
    "            class_mode_vals = []\n",
    "            class_bbox_areas = []\n",
    "            for x1, y1, x2, y2 in coords:\n",
    "                # Load the Midas depth image\n",
    "                midas_img = cv2.imread(midas_img_path)\n",
    "                midas_img = cv2.cvtColor(midas_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Preprocess the image for the Midas model\n",
    "                input_batch = transform(midas_img).to(device)\n",
    "\n",
    "                # Make a prediction using the Midas model\n",
    "                with torch.no_grad():\n",
    "                    prediction = midas(input_batch)\n",
    "\n",
    "                    prediction = torch.nn.functional.interpolate(\n",
    "                        prediction.unsqueeze(1),\n",
    "                        size=midas_img.shape[:2],\n",
    "                        mode=\"bicubic\",\n",
    "                        align_corners=False,\n",
    "                    ).squeeze()\n",
    "\n",
    "                # Convert the prediction to a numpy array\n",
    "                midas_output = prediction.cpu().numpy()\n",
    "\n",
    "                # Crop the corresponding region from the Midas output\n",
    "                midas_roi = midas_output[y1:y2, x1:x2]\n",
    "\n",
    "                # Calculate the mode of the pixel values in the ROI\n",
    "                mode_val = np.argmax(np.bincount(midas_roi.flatten()))\n",
    "                class_mode_vals.append(mode_val)\n",
    "\n",
    "                # Calculate the area of the bounding box\n",
    "                bbox_area = bbox_area = (x2 - x1) * (y2 - y1)\n",
    "            class_bbox_areas.append(bbox_area)\n",
    "\n",
    "        # Calculate the mode value and area of the bounding box for the class\n",
    "        mode_val = np.argmax(np.bincount(class_mode_vals))\n",
    "        bbox_area = np.median(class_bbox_areas)\n",
    "        class_results = {'mode_value': mode_val, 'bbox_area': bbox_area}\n",
    "\n",
    "        # Calculate the weight of the class\n",
    "        class_weight = scale_factor * density[class_id] * bbox_area * mode_val\n",
    "        class_results['weight'] = class_weight\n",
    "\n",
    "        # Add the weight of the class to the total weight of the image\n",
    "        total_weight += class_weight\n",
    "\n",
    "        file_results[class_id] = class_results\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    results[filename] = file_results\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
